# arXiv Abstract Topic Classifier

This project investigates whether a fine-tuned BERT model can accurately classify scientific papers into subject categories using only the abstract text. Using arXiv metadata, we constructed a labeled dataset with stratified train/validation/test splits, while preserving the natural class imbalance of academic fields. After minimal preprocessing and tokenization, we fine-tuned bert-base-uncased for multi-class classification. Evaluation on the test set shows strong results, with overall accuracy of about 82% and macro-F1 near 0.79
. High-performing categories include Astrophysics, Nuclear Physics, Quantitative Finance, and Neuroscience, each benefiting from distinctive terminology and sufficient training data. Misclassifications mostly occur between semantically adjacent domains such as physics subfields or related areas in computer science, highlighting both the power and the limits of abstract-only classification. The findings suggest that transformer-based models can effectively leverage concise abstracts for large-scale categorization, while future improvements may come from hierarchical modeling, domain-adaptive pretraining, or incorporating additional metadata.
